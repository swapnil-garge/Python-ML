CUSUM: Your original method with positive/negative shift detection
Changepoint Detection: Ruptures library with PELT algorithm
Statistical Tests: t-tests and F-tests for mean/variance shifts
EWMA Control Charts: Superior for small persistent shifts
Time Series Clustering: Identifies operational regime changes
Bayesian Changepoint: Probabilistic approach without heavy dependencies

2. Consensus Approach:

Combines results from multiple methods
Reduces false positives
Provides confidence levels for detected shifts
Configurable tolerance (days) and minimum methods required

3. Comprehensive Visualization:

Original CUSUM charts preserved
New comprehensive comparison plots showing all 6 methods
Performance heatmap across product-target combinations
Method-specific visualizations

4. Manufacturing-Specific Features:

Product-wise analysis (as requested by operations)
Data volume prioritization (focuses on products with most data)
Lag relationship ready (can be extended for your 2-7 day material lags)
Missing value handling
Export capabilities for operations teams

5. Why This Approach is Superior:
For Your Manufacturing Context:

Multiple validation: Consensus reduces false alarms critical in manufacturing
Gradual shift detection: EWMA and statistical methods catch subtle changes CUSUM misses
Regime identification: Clustering identifies different production modes
Probabilistic confidence: Bayesian provides uncertainty quantification
Operational insights: Product-specific analysis aligns with your operations team needs

Next Steps for Your Analysis:

Lag Integration: Extend to create lagged features for materials (X11-X44) and pre-step variables (X5-X8, etc.)
Root Cause Correlation: Once shifts are detected, correlate with process variables
Predictive Modeling: Use shift probabilities to predict future utilization issues
Shutdown Integration: Filter out planned maintenance periods from analysis

This integrated solution gives you the robustness of multiple methods while maintaining the familiar CUSUM approach your team knows, providing a comprehensive foundation for identifying the root causes of low machine utilization.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import ruptures as rpt
from statsmodels.tsa.stattools import adfuller
from statsmodels.stats.diagnostic import het_white
import warnings
warnings.filterwarnings('ignore')

# For Bayesian changepoint detection
try:
    import pymc3 as pm
    import theano.tensor as tt
    BAYESIAN_AVAILABLE = True
except ImportError:
    try:
        # Alternative lightweight Bayesian implementation
        from scipy.special import gammaln, loggamma
        BAYESIAN_AVAILABLE = True
    except ImportError:
        BAYESIAN_AVAILABLE = False

class ComprehensiveShiftDetection:
    """
    Comprehensive shift detection suite combining CUSUM with advanced methods
    """
    
    def __init__(self, df, target_vars, date_col='GMT Prod Yield Date', product_col='Merge'):
        self.df = df.copy()
        self.target_vars = target_vars
        self.date_col = date_col
        self.product_col = product_col
        self.shift_results = {}
        
    def detect_all_shifts(self, products=None, methods=['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian'], 
                         cusum_params={'threshold': 3, 'drift': 0.5}):
        """Apply all shift detection methods"""
        if products is None:
            # Select top products by data volume
            product_sizes = {}
            for product in self.df[self.product_col].unique():
                prod_data = self.df[self.df[self.product_col] == product]
                product_sizes[product] = len(prod_data)
            
            sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
            products = [prod for prod, _ in sorted_products[:10]]  # Top 10 products
            
        for product in products:
            self.shift_results[product] = {}
            product_data = self.df[self.df[self.product_col] == product].copy()
            
            if len(product_data) < 20:  # Skip products with insufficient data
                continue
                
            for target in self.target_vars:
                if target not in product_data.columns:
                    continue
                    
                series = product_data.set_index(self.date_col)[target].dropna()
                if len(series) < 20:
                    continue
                
                self.shift_results[product][target] = {}
                
                # Apply each method
                if 'cusum' in methods:
                    self.shift_results[product][target]['cusum'] = self._cusum_detection(
                        series, cusum_params['threshold'], cusum_params['drift'])
                
                if 'changepoint' in methods:
                    self.shift_results[product][target]['changepoint'] = self._changepoint_detection(series)
                
                if 'statistical' in methods:
                    self.shift_results[product][target]['statistical'] = self._statistical_tests(series)
                
                if 'ewma' in methods:
                    self.shift_results[product][target]['ewma'] = self._ewma_control(series)
                    
                if 'clustering' in methods:
                    self.shift_results[product][target]['clustering'] = self._time_series_clustering(series)
                
                if 'bayesian' in methods and BAYESIAN_AVAILABLE:
                    self.shift_results[product][target]['bayesian'] = self._bayesian_changepoint(series)
    
    def _cusum_detection(self, series, threshold=3, drift=0.5):
        """
        CUSUM detection method - integrated from original code
        """
        results = {'shifts': {'positive_shifts': [], 'negative_shifts': []}, 
                  'method': 'cusum', 'cusum_stats': {}}
        
        try:
            # Standardize
            mean_val = series.mean()
            std_val = series.std()
            if std_val == 0:
                return results
            
            standardized = (series - mean_val) / std_val
            
            # Calculate CUSUM
            cusum_pos = np.zeros(len(standardized))
            cusum_neg = np.zeros(len(standardized))
            
            positive_shifts = []
            negative_shifts = []
            
            for j in range(1, len(standardized)):
                cusum_pos[j] = max(0, cusum_pos[j-1] + standardized.iloc[j] - drift)
                cusum_neg[j] = min(0, cusum_neg[j-1] + standardized.iloc[j] + drift)
                
                # Detect shifts
                if cusum_pos[j] > threshold and cusum_pos[j-1] <= threshold:
                    positive_shifts.append(j)
                if cusum_neg[j] < -threshold and cusum_neg[j-1] >= -threshold:
                    negative_shifts.append(j)
            
            # Convert indices to dates
            positive_shift_dates = [series.index[idx] for idx in positive_shifts if idx < len(series)]
            negative_shift_dates = [series.index[idx] for idx in negative_shifts if idx < len(series)]
            
            results['shifts'] = {
                'positive_shifts': positive_shift_dates,
                'negative_shifts': negative_shift_dates,
                'all_shifts': positive_shift_dates + negative_shift_dates
            }
            results['cusum_stats'] = {
                'cusum_pos': cusum_pos,
                'cusum_neg': cusum_neg,
                'threshold': threshold,
                'drift': drift,
                'mean': mean_val,
                'std': std_val
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _changepoint_detection(self, series, methods=['pelt', 'window']):
        """
        Ruptures library changepoint detection - excellent for manufacturing data
        """
        results = {'shifts': [], 'method': 'changepoint', 'scores': []}
        
        try:
            signal = series.values
            
            # PELT algorithm for multiple changepoints
            algo_pelt = rpt.Pelt(model="rbf", min_size=5, jump=1).fit(signal)
            changepoints_pelt = algo_pelt.predict(pen=10)
            
            if changepoints_pelt and changepoints_pelt[-1] == len(signal):
                changepoints_pelt = changepoints_pelt[:-1]
            
            pelt_dates = [series.index[cp] for cp in changepoints_pelt if cp < len(series)]
            
            # Window-based detection
            algo_window = rpt.Window(width=10, model="l2").fit(signal)
            changepoints_window = algo_window.predict(n_bkps=3)
            if changepoints_window and changepoints_window[-1] == len(signal):
                changepoints_window = changepoints_window[:-1]
            
            window_dates = [series.index[cp] for cp in changepoints_window if cp < len(series)]
            
            results['shifts'] = {
                'pelt': pelt_dates,
                'window': window_dates,
                'combined': list(set(pelt_dates + window_dates))
            }
            results['scores'] = {'pelt_penalty': 10, 'n_changepoints': len(pelt_dates)}
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _statistical_tests(self, series, window_size=15):
        """
        Statistical shift detection using t-tests and variance tests
        """
        results = {'shifts': [], 'method': 'statistical', 'test_stats': []}
        
        try:
            values = series.values
            dates = series.index
            shifts = []
            test_stats = []
            
            for i in range(window_size, len(values) - window_size):
                before = values[i-window_size:i]
                after = values[i:i+window_size]
                
                # T-test for mean shift
                t_stat, t_p = stats.ttest_ind(before, after, equal_var=False)
                
                # F-test for variance shift
                f_stat = np.var(after, ddof=1) / np.var(before, ddof=1)
                f_p = 1 - stats.f.cdf(f_stat, len(after)-1, len(before)-1)
                
                if t_p < 0.01 or f_p < 0.01:
                    shifts.append(dates[i])
                    test_stats.append({
                        'date': dates[i],
                        't_stat': t_stat,
                        't_p': t_p,
                        'f_stat': f_stat,
                        'f_p': f_p,
                        'shift_magnitude': np.mean(after) - np.mean(before)
                    })
            
            results['shifts'] = shifts
            results['test_stats'] = test_stats
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _ewma_control(self, series, lambda_ewma=0.2, threshold_sigma=3):
        """
        Exponentially Weighted Moving Average control chart
        """
        results = {'shifts': [], 'method': 'ewma', 'control_stats': {}}
        
        try:
            values = series.values
            dates = series.index
            
            # Calculate EWMA
            ewma = np.zeros(len(values))
            ewma[0] = values[0]
            
            for i in range(1, len(values)):
                ewma[i] = lambda_ewma * values[i] + (1 - lambda_ewma) * ewma[i-1]
            
            # Control limits
            overall_mean = np.mean(values)
            overall_std = np.std(values)
            
            ewma_var = []
            for i in range(len(values)):
                var_i = (overall_std**2) * (lambda_ewma / (2 - lambda_ewma)) * (1 - (1 - lambda_ewma)**(2*(i+1)))
                ewma_var.append(var_i)
            
            ewma_std = np.sqrt(ewma_var)
            ucl = overall_mean + threshold_sigma * ewma_std
            lcl = overall_mean - threshold_sigma * ewma_std
            
            # Detect shifts
            shifts = []
            for i in range(1, len(ewma)):
                if ewma[i] > ucl[i] or ewma[i] < lcl[i]:
                    shifts.append(dates[i])
            
            results['shifts'] = shifts
            results['control_stats'] = {
                'ewma': ewma,
                'ucl': ucl,
                'lcl': lcl,
                'center_line': overall_mean,
                'lambda': lambda_ewma
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _time_series_clustering(self, series, n_clusters=3, window_size=10):
        """
        Time series clustering approach to identify regime changes
        """
        results = {'shifts': [], 'method': 'clustering', 'cluster_info': {}}
        
        try:
            values = series.values
            dates = series.index
            
            windows = []
            window_dates = []
            
            for i in range(window_size, len(values)):
                window = values[i-window_size:i]
                windows.append([
                    np.mean(window),
                    np.std(window),
                    np.max(window) - np.min(window),
                    np.percentile(window, 75) - np.percentile(window, 25)
                ])
                window_dates.append(dates[i])
            
            if len(windows) < n_clusters:
                results['error'] = 'Insufficient data for clustering'
                return results
            
            scaler = StandardScaler()
            windows_scaled = scaler.fit_transform(windows)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(windows_scaled)
            
            shifts = []
            for i in range(1, len(clusters)):
                if clusters[i] != clusters[i-1]:
                    shifts.append(window_dates[i])
            
            results['shifts'] = shifts
            results['cluster_info'] = {
                'clusters': clusters,
                'cluster_centers': kmeans.cluster_centers_,
                'silhouette_score': silhouette_score(windows_scaled, clusters),
                'n_clusters': n_clusters
            }
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def _bayesian_changepoint(self, series, max_changepoints=5):
        """
        Bayesian changepoint detection using Student's t-test approach
        Lightweight implementation without requiring PyMC3
        """
        results = {'shifts': [], 'method': 'bayesian', 'probabilities': []}
        
        try:
            if not BAYESIAN_AVAILABLE:
                results['error'] = 'Bayesian libraries not available'
                return results
            
            values = series.values
            dates = series.index
            n = len(values)
            
            if n < 20:
                results['error'] = 'Insufficient data for Bayesian analysis'
                return results
            
            # Simple Bayesian changepoint detection using likelihood ratios
            changepoint_probs = []
            
            for t in range(10, n-10):  # Avoid edges
                # Calculate likelihood of no changepoint
                full_mean = np.mean(values)
                full_var = np.var(values, ddof=1)
                
                if full_var == 0:
                    continue
                
                # Log-likelihood for full series (no changepoint)
                ll_no_change = -0.5 * n * np.log(2 * np.pi * full_var) - 0.5 * np.sum((values - full_mean)**2) / full_var
                
                # Calculate likelihood with changepoint at t
                mean_1 = np.mean(values[:t])
                var_1 = np.var(values[:t], ddof=1) if t > 1 else full_var
                mean_2 = np.mean(values[t:])
                var_2 = np.var(values[t:], ddof=1) if n-t > 1 else full_var
                
                if var_1 == 0:
                    var_1 = full_var
                if var_2 == 0:
                    var_2 = full_var
                
                # Log-likelihood for two segments
                ll_1 = -0.5 * t * np.log(2 * np.pi * var_1) - 0.5 * np.sum((values[:t] - mean_1)**2) / var_1
                ll_2 = -0.5 * (n-t) * np.log(2 * np.pi * var_2) - 0.5 * np.sum((values[t:] - mean_2)**2) / var_2
                ll_change = ll_1 + ll_2
                
                # Bayes factor (simplified, without proper priors)
                bayes_factor = np.exp(ll_change - ll_no_change)
                
                # Convert to probability (rough approximation)
                prob_changepoint = bayes_factor / (1 + bayes_factor)
                changepoint_probs.append((t, prob_changepoint))
            
            # Select changepoints with high probability
            changepoint_probs.sort(key=lambda x: x[1], reverse=True)
            
            # Threshold for considering a changepoint significant
            threshold_prob = 0.7
            significant_changepoints = []
            
            for t, prob in changepoint_probs[:max_changepoints]:
                if prob > threshold_prob:
                    # Check that it's not too close to already selected changepoints
                    too_close = False
                    for existing_t, _ in significant_changepoints:
                        if abs(t - existing_t) < 10:  # Minimum 10 points apart
                            too_close = True
                            break
                    
                    if not too_close:
                        significant_changepoints.append((t, prob))
            
            # Convert to dates
            shifts = [dates[t] for t, prob in significant_changepoints]
            
            results['shifts'] = shifts
            results['probabilities'] = significant_changepoints
            results['all_probs'] = changepoint_probs
            
        except Exception as e:
            results['error'] = str(e)
            
        return results
    
    def consensus_shifts(self, product, target, min_methods=2, tolerance_days=7):
        """
        Find shifts detected by multiple methods (consensus approach)
        """
        if product not in self.shift_results or target not in self.shift_results[product]:
            return []
        
        all_shifts = []
        method_data = self.shift_results[product][target]
        
        # Collect all shift dates from different methods
        for method_name, method_result in method_data.items():
            if 'error' in method_result:
                continue
                
            if 'shifts' in method_result:
                if method_name == 'changepoint':
                    shifts = method_result['shifts'].get('combined', [])
                elif method_name == 'cusum':
                    shifts = method_result['shifts'].get('all_shifts', [])
                elif isinstance(method_result['shifts'], list):
                    shifts = method_result['shifts']
                else:
                    continue
                all_shifts.extend(shifts)
        
        if not all_shifts:
            return []
        
        # Convert to pandas datetime
        shift_series = pd.to_datetime(all_shifts)
        
        # Group shifts within tolerance days of each other
        consensus_shifts = []
        tolerance = pd.Timedelta(days=tolerance_days)
        
        used_shifts = set()
        
        for shift in shift_series:
            if shift in used_shifts:
                continue
            
            # Find all shifts within tolerance
            nearby_shifts = shift_series[abs(shift_series - shift) <= tolerance]
            
            if len(nearby_shifts) >= min_methods:
                consensus_shifts.append(shift)
                used_shifts.update(nearby_shifts)
        
        return sorted(consensus_shifts)
    
    def plot_cusum_charts(self, product=None, target_var=None, threshold=3, drift=0.5, save_plots=True, max_products=5):
        """
        Create comprehensive CUSUM charts - integrated from original code
        """
        available_products = list(self.shift_results.keys())
        if product is None:
            product_sizes = {}
            for prod in available_products:
                prod_data = self.df[self.df[self.product_col] == prod]
                product_sizes[prod] = len(prod_data)
            
            sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
            products = [prod for prod, _ in sorted_products[:max_products]]
        else:
            products = [product] if product in available_products else available_products[:1]
        
        if target_var is None:
            targets = self.target_vars
        else:
            targets = [target_var] if target_var in self.target_vars else self.target_vars
        
        for target in targets:
            if target not in self.df.columns:
                continue
                
            n_products = len(products)
            if n_products == 0:
                continue
                
            fig, axes = plt.subplots(n_products, 3, figsize=(20, 5*n_products))
            if n_products == 1:
                axes = axes.reshape(1, -1)
            
            fig.suptitle(f'CUSUM Analysis for {target} (Top {n_products} Products by Data Volume)', 
                        fontsize=16, fontweight='bold')
            
            for i, product in enumerate(products):
                product_data = self.df[self.df[self.product_col] == product].copy()
                
                if target not in product_data.columns or product_data[target].isna().all():
                    continue
                
                series = product_data.set_index(self.date_col)[target].dropna()
                if len(series) < 10:
                    continue
                
                # Get CUSUM results
                if product in self.shift_results and target in self.shift_results[product] and 'cusum' in self.shift_results[product][target]:
                    cusum_result = self.shift_results[product][target]['cusum']
                    cusum_stats = cusum_result.get('cusum_stats', {})
                    cusum_pos = cusum_stats.get('cusum_pos', np.zeros(len(series)))
                    cusum_neg = cusum_stats.get('cusum_neg', np.zeros(len(series)))
                    mean_val = cusum_stats.get('mean', series.mean())
                    std_val = cusum_stats.get('std', series.std())
                else:
                    # Calculate CUSUM if not available
                    mean_val = series.mean()
                    std_val = series.std()
                    standardized = (series - mean_val) / std_val
                    cusum_pos = np.zeros(len(standardized))
                    cusum_neg = np.zeros(len(standardized))
                    
                    for j in range(1, len(standardized)):
                        cusum_pos[j] = max(0, cusum_pos[j-1] + standardized.iloc[j] - drift)
                        cusum_neg[j] = min(0, cusum_neg[j-1] + standardized.iloc[j] + drift)
                
                # Plot 1: Original Time Series
                ax1 = axes[i, 0]
                ax1.plot(series.index, series.values, 'b-', linewidth=1.5, label='Actual Data')
                ax1.axhline(y=mean_val, color='black', linestyle='-', alpha=0.8, label='Mean')
                ax1.axhline(y=mean_val + 3*std_val, color='red', linestyle='--', alpha=0.7, label='UCL (+3σ)')
                ax1.axhline(y=mean_val - 3*std_val, color='red', linestyle='--', alpha=0.7, label='LCL (-3σ)')
                
                # Mark detected shifts
                if (product in self.shift_results and target in self.shift_results[product] 
                    and 'cusum' in self.shift_results[product][target]):
                    shifts = self.shift_results[product][target]['cusum']['shifts']
                    for shift_date in shifts.get('positive_shifts', []):
                        ax1.axvline(shift_date, color='red', linestyle='--', alpha=0.8, linewidth=2)
                    for shift_date in shifts.get('negative_shifts', []):
                        ax1.axvline(shift_date, color='green', linestyle='--', alpha=0.8, linewidth=2)
                
                ax1.set_title(f'{product} - {target}\nOriginal Time Series (n={len(series)})')
                ax1.set_ylabel(target)
                ax1.legend(loc='upper right', fontsize=8)
                ax1.grid(True, alpha=0.3)
                plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 2: Positive CUSUM
                ax2 = axes[i, 1]
                ax2.plot(series.index, cusum_pos, 'r-', linewidth=2, label='Positive CUSUM')
                ax2.axhline(y=threshold, color='red', linestyle='--', alpha=0.8, label=f'Threshold ({threshold})')
                ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                above_threshold = cusum_pos > threshold
                if np.any(above_threshold):
                    ax2.fill_between(series.index, 0, cusum_pos, where=above_threshold, 
                                   color='red', alpha=0.3, label='Above Threshold')
                
                ax2.set_title(f'Positive CUSUM (Upward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax2.set_ylabel('Positive CUSUM')
                ax2.legend(loc='upper right', fontsize=8)
                ax2.grid(True, alpha=0.3)
                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 3: Negative CUSUM
                ax3 = axes[i, 2]
                ax3.plot(series.index, cusum_neg, 'g-', linewidth=2, label='Negative CUSUM')
                ax3.axhline(y=-threshold, color='green', linestyle='--', alpha=0.8, label=f'Threshold (-{threshold})')
                ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                below_threshold = cusum_neg < -threshold
                if np.any(below_threshold):
                    ax3.fill_between(series.index, 0, cusum_neg, where=below_threshold, 
                                   color='green', alpha=0.3, label='Below Threshold')
                
                ax3.set_title(f'Negative CUSUM (Downward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax3.set_ylabel('Negative CUSUM')
                ax3.legend(loc='lower right', fontsize=8)
                ax3.grid(True, alpha=0.3)
                plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
            
            plt.tight_layout()
            if save_plots:
                plt.savefig(f'cusum_analysis_{target.replace(" ", "_").replace("%", "pct")}.png', 
                           dpi=300, bbox_inches='tight')
            plt.show()
    
    def plot_comprehensive_comparison(self, product, target, save_plots=True):
        """
        Plot comparison of ALL methods including CUSUM and Bayesian
        """
        if product not in self.shift_results or target not in self.shift_results[product]:
            print(f"No results found for {product} - {target}")
            return
        
        product_data = self.df[self.df[self.product_col] == product].copy()
        series = product_data.set_index(self.date_col)[target].dropna()
        
        if len(series) == 0:
            return
        
        # Create the plot with 6 subplots for all methods
        fig, axes = plt.subplots(6, 1, figsize=(16, 16))
        fig.suptitle(f'Comprehensive Shift Detection: {product} - {target}', fontsize=14, fontweight='bold')
        
        method_data = self.shift_results[product][target]
        
        # Plot 1: CUSUM
        ax1 = axes[0]
        ax1.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'cusum' in method_data and 'cusum_stats' in method_data['cusum']:
            cusum_stats = method_data['cusum']['cusum_stats']
            # Plot CUSUM on secondary axis
            ax1_twin = ax1.twinx()
            ax1_twin.plot(series.index, cusum_stats['cusum_pos'], 'r--', alpha=0.7, label='Pos CUSUM')
            ax1_twin.plot(series.index, cusum_stats['cusum_neg'], 'g--', alpha=0.7, label='Neg CUSUM')
            ax1_twin.axhline(y=cusum_stats['threshold'], color='red', linestyle=':', alpha=0.5)
            ax1_twin.axhline(y=-cusum_stats['threshold'], color='green', linestyle=':', alpha=0.5)
            ax1_twin.set_ylabel('CUSUM Values')
            
            for shift in method_data['cusum']['shifts']['all_shifts']:
                ax1.axvline(shift, color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        ax1.set_title('CUSUM Detection')
        ax1.set_ylabel(target)
        ax1.legend(loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Changepoint Detection
        ax2 = axes[1]
        ax2.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'changepoint' in method_data and 'shifts' in method_data['changepoint']:
            changepoints = method_data['changepoint']['shifts'].get('combined', [])
            for cp in changepoints:
                ax2.axvline(cp, color='red', linestyle='--', alpha=0.7, linewidth=2)
        
        ax2.set_title('Changepoint Detection (Ruptures)')
        ax2.set_ylabel(target)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Statistical Tests
        ax3 = axes[2]
        ax3.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'statistical' in method_data and 'shifts' in method_data['statistical']:
            for shift in method_data['statistical']['shifts']:
                ax3.axvline(shift, color='green', linestyle='--', alpha=0.7, linewidth=2)
        
        ax3.set_title('Statistical Tests (t-test & F-test)')
        ax3.set_ylabel(target)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: EWMA
        ax4 = axes[3]
        ax4.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'ewma' in method_data and 'control_stats' in method_data['ewma']:
            ewma_stats = method_data['ewma']['control_stats']
            ax4.plot(series.index, ewma_stats['ewma'], 'orange', linewidth=2, label='EWMA')
            ax4.plot(series.index, ewma_stats['ucl'], 'r--', alpha=0.7, label='UCL')
            ax4.plot(series.index, ewma_stats['lcl'], 'r--', alpha=0.7, label='LCL')
            
            for shift in method_data['ewma']['shifts']:
                ax4.axvline(shift, color='orange', linestyle='--', alpha=0.7, linewidth=2)
        
        ax4.set_title('EWMA Control Chart')
        ax4.set_ylabel(target)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # Plot 5: Clustering
        ax5 = axes[4]
        ax5.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'clustering' in method_data and 'shifts' in method_data['clustering']:
            for shift in method_data['clustering']['shifts']:
                ax5.axvline(shift, color='purple', linestyle='--', alpha=0.7, linewidth=2)
        
        ax5.set_title('Time Series Clustering')
        ax5.set_ylabel(target)
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # Plot 6: Bayesian + Consensus
        ax6 = axes[5]
        ax6.plot(series.index, series.values, 'b-', linewidth=1.5, alpha=0.8, label='Original Data')
        
        if 'bayesian' in method_data and 'shifts' in method_data['bayesian']:
            for shift in method_data['bayesian']['shifts']:
                ax6.axvline(shift, color='brown', linestyle='--', alpha=0.7, linewidth=2, label='Bayesian')
        
        # Plot consensus shifts
        consensus = self.consensus_shifts(product, target)
        for shift in consensus:
            ax6.axvline(shift, color='black', linestyle='-', alpha=0.9, linewidth=3, label='Consensus')
        
        ax6.set_title('Bayesian Detection + Consensus Shifts')
        ax6.set_ylabel(target)
        ax6.set_xlabel('Date')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        if save_plots:
            filename = f'comprehensive_shift_analysis_{product}_{target}'.replace(' ', '_').replace('%', 'pct')
            plt.savefig(f'{filename}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_method_performance_heatmap(self, save_plots=True):
        """
        Create heatmap showing number of shifts detected by each method for each product-target combination
        """
        # Collect data for heatmap
        products = list(self.shift_results.keys())
        methods = ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian']
        
        heatmap_data = []
        labels = []
        
        for product in products:
            for target in self.target_vars:
                if target in self.shift_results[product]:
                    row = []
                    for method in methods:
                        if method in self.shift_results[product][target]:
                            method_result = self.shift_results[product][target][method]
                            if 'error' in method_result:
                                count = 0
                            elif method == 'cusum':
                                count = len(method_result['shifts'].get('all_shifts', []))
                            elif method == 'changepoint':
                                count = len(method_result['shifts'].get('combined', []))
                            else:
                                count = len(method_result.get('shifts', []))
                        else:
                            count = 0
                        row.append(count)
                    
                    # Add consensus count
                    consensus_count = len(self.consensus_shifts(product, target))
                    row.append(consensus_count)
                    
                    heatmap_data.append(row)
                    labels.append(f'{product[:8]}_{target}')
        
        if not heatmap_data:
            print("No data available for heatmap")
            return
        
        # Create heatmap
        fig, ax = plt.subplots(figsize=(10, max(8, len(labels)*0.3)))
        
        heatmap_array = np.array(heatmap_data)
        im = ax.imshow(heatmap_array, cmap='YlOrRd', aspect='auto')
        
        # Set ticks and labels
        ax.set_xticks(range(len(methods) + 1))
        ax.set_xticklabels(methods + ['Consensus'])
        ax.set_yticks(range(len(labels)))
        ax.set_yticklabels(labels)
        
        # Add text annotations
        for i in range(len(labels)):
            for j in range(len(methods) + 1):
                text = ax.text(j, i, int(heatmap_array[i, j]), 
                             ha="center", va="center", color="black", fontsize=8)
        
        ax.set_title('Number of Shifts Detected by Method\n(Product_Target combinations)', fontweight='bold')
        plt.colorbar(im)
        plt.tight_layout()
        
        if save_plots:
            plt.savefig('method_performance_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def summary_report(self):
        """
        Generate comprehensive summary report of all detected shifts
        """
        report = []
        
        for product in self.shift_results:
            for target in self.shift_results[product]:
                consensus_shifts = self.consensus_shifts(product, target)
                
                method_counts = {}
                method_errors = {}
                
                for method_name, method_result in self.shift_results[product][target].items():
                    if 'error' in method_result:
                        method_errors[method_name] = method_result['error']
                        method_counts[method_name] = 0
                    elif 'shifts' in method_result:
                        if method_name == 'cusum':
                            count = len(method_result['shifts'].get('all_shifts', []))
                        elif method_name == 'changepoint':
                            count = len(method_result['shifts'].get('combined', []))
                        else:
                            count = len(method_result['shifts'])
                        method_counts[method_name] = count
                
                # Calculate data quality metrics
                product_data = self.df[self.df[self.product_col] == product]
                series = product_data.set_index(self.date_col)[target].dropna()
                
                report.append({
                    'Product': product,
                    'Target': target,
                    'Data_Points': len(series),
                    'Date_Range': f"{series.index.min()} to {series.index.max()}",
                    'Consensus_Shifts': len(consensus_shifts),
                    'Consensus_Dates': [str(d.date()) for d in consensus_shifts],
                    **{f'{method}_count': method_counts.get(method, 0) 
                       for method in ['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian']},
                    'Errors': '; '.join([f'{k}: {v}' for k, v in method_errors.items()]) if method_errors else 'None'
                })
        
        return pd.DataFrame(report)
    
    def export_shift_details(self, filename='shift_detection_results.xlsx'):
        """
        Export detailed results to Excel with multiple sheets
        """
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Summary sheet
            summary = self.summary_report()
            summary.to_excel(writer, sheet_name='Summary', index=False)
            
            # Detailed results for each method
            for product in self.shift_results:
                for target in self.shift_results[product]:
                    sheet_name = f'{product[:10]}_{target[:10]}'.replace(' ', '_').replace('%', 'pct')
                    
                    detailed_data = []
                    
                    for method_name, method_result in self.shift_results[product][target].items():
                        if 'error' in method_result:
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Error',
                                'Details': method_result['error'],
                                'Shift_Count': 0,
                                'Shift_Dates': ''
                            })
                        elif 'shifts' in method_result:
                            if method_name == 'cusum':
                                shifts = method_result['shifts'].get('all_shifts', [])
                            elif method_name == 'changepoint':
                                shifts = method_result['shifts'].get('combined', [])
                            else:
                                shifts = method_result.get('shifts', [])
                            
                            detailed_data.append({
                                'Method': method_name,
                                'Status': 'Success',
                                'Details': f"Detected {len(shifts)} shifts",
                                'Shift_Count': len(shifts),
                                'Shift_Dates': '; '.join([str(d.date()) if hasattr(d, 'date') else str(d) for d in shifts])
                            })
                    
                    # Add consensus row
                    consensus = self.consensus_shifts(product, target)
                    detailed_data.append({
                        'Method': 'Consensus',
                        'Status': 'Computed',
                        'Details': f"Consensus from {len(detailed_data)} methods",
                        'Shift_Count': len(consensus),
                        'Shift_Dates': '; '.join([str(d.date()) for d in consensus])
                    })
                    
                    if detailed_data:
                        detail_df = pd.DataFrame(detailed_data)
                        detail_df.to_excel(writer, sheet_name=sheet_name, index=False)


# Example usage and workflow
"""
# Initialize the comprehensive shift detection suite
detector = ComprehensiveShiftDetection(df, target_vars=['Y1', 'Y2', 'Y3', 'Y4', 'Y5'])

# Run all shift detection methods
detector.detect_all_shifts(methods=['cusum', 'changepoint', 'statistical', 'ewma', 'clustering', 'bayesian'])

# Generate summary report
summary = detector.summary_report()
print("SHIFT DETECTION SUMMARY:")
print(summary.to_string())

# Create CUSUM charts (original functionality)
detector.plot_cusum_charts(max_products=3)

# Create comprehensive comparison plots
top_products = summary.nlargest(3, 'Data_Points')['Product'].unique()
for product in top_products:
    for target in ['Y1', 'Y2']:  # Focus on key targets
        detector.plot_comprehensive_comparison(product, target)

# Create performance heatmap
detector.plot_method_performance_heatmap()

# Export detailed results
detector.export_shift_details('comprehensive_shift_analysis.xlsx')

# Get consensus shifts for specific cases
for product in top_products[:2]:
    for target in ['Y1', 'Y2']:
        consensus = detector.consensus_shifts(product, target, min_methods=3)
        if consensus:
            print(f"\n{product} - {target} CONSENSUS SHIFTS:")
            for shift_date in consensus:
                print(f"  {shift_date.date()}")
"""
