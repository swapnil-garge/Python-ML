import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from scipy import stats
from scipy.signal import find_peaks
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class OEEShiftAnalyzer:
    """
    Comprehensive analyzer for identifying shifts in OEE target variables 
    and correlating with process/material changes
    """
    
    def __init__(self, data_path=None, df=None):
        if df is not None:
            self.df = df.copy()
        else:
            self.df = pd.read_csv(data_path)
        
        # Define variable categories
        self.target_vars = ['Downtime (Hrs)', 'Uptime', 'FPY %', 'Wound %', 'OEE Availability']
        self.material_vars = ['X11', 'X12', 'X13', 'X14', 'X15', 'X17', 'X24'] + [f'X{i}' for i in range(38, 45)]
        self.prestep_vars = ['X5', 'X6', 'X7', 'X8', 'X18', 'X19', 'X25', 'X26', 'X28', 'X29', 'X34', 'X35', 'X36']
        
        # Process vars (X1-X33 excluding material and prestep)
        all_x_vars = [f'X{i}' for i in range(1, 34)]
        self.process_vars = [x for x in all_x_vars if x not in self.material_vars + self.prestep_vars]
        
        self.string_vars = ['X14', 'X17']
        self.shift_results = {}
        self.correlation_results = {}
        
    def preprocess_data(self):
        """Clean and prepare data for analysis"""
        print("Preprocessing data...")
        
        # Convert date column
        self.df['GMT Prod Yield Date'] = pd.to_datetime(self.df['GMT Prod Yield Date'])
        self.df = self.df.sort_values('GMT Prod Yield Date').reset_index(drop=True)
        
        # Handle string variables - create dummy variables
        for var in self.string_vars:
            if var in self.df.columns:
                dummies = pd.get_dummies(self.df[var], prefix=var, dummy_na=True)
                self.df = pd.concat([self.df, dummies], axis=1)
        
        # Create lagged variables for material and prestep
        self.create_lagged_variables()
        
        print(f"Data shape after preprocessing: {self.df.shape}")
        return self.df
    
    def create_lagged_variables(self):
        """Create lagged versions of material and prestep variables"""
        print("Creating lagged variables...")
        
        # Material variables: 2-7 days lag
        for var in self.material_vars:
            if var in self.df.columns and var not in self.string_vars:
                for lag in range(2, 8):
                    self.df[f'{var}_lag{lag}'] = self.df[var].shift(lag)
        
        # Prestep variables: 1 day lag (8-16 hours â‰ˆ 1 day)
        for var in self.prestep_vars:
            if var in self.df.columns:
                self.df[f'{var}_lag1'] = self.df[var].shift(1)
    
    def detect_shifts_cusum(self, series, threshold=3, drift=0.5):
        """
        Detect shifts using CUSUM (Cumulative Sum) method
        
        Parameters:
        - series: time series data
        - threshold: detection threshold (higher = less sensitive)
        - drift: allowable drift (smaller = more sensitive)
        """
        series_clean = series.dropna()
        if len(series_clean) < 10:
            return [], []
        
        # Standardize the series
        mean_val = series_clean.mean()
        std_val = series_clean.std()
        if std_val == 0:
            return [], []
        
        standardized = (series_clean - mean_val) / std_val
        
        # CUSUM calculation
        cusum_pos = np.zeros(len(standardized))
        cusum_neg = np.zeros(len(standardized))
        
        for i in range(1, len(standardized)):
            cusum_pos[i] = max(0, cusum_pos[i-1] + standardized.iloc[i] - drift)
            cusum_neg[i] = min(0, cusum_neg[i-1] + standardized.iloc[i] + drift)
        
        # Detect shifts - return integer indices
        pos_shifts = find_peaks(cusum_pos, height=threshold)[0]
        neg_shifts = find_peaks(-cusum_neg, height=threshold)[0]
        
        return pos_shifts.tolist(), neg_shifts.tolist()
    
    def detect_shifts_changepoint(self, series, min_size=10):
        """
        Alternative shift detection using statistical change point detection
        """
        series_clean = series.dropna()
        if len(series_clean) < 2 * min_size:
            return []
        
        change_points = []
        n = len(series_clean)
        
        # Sliding window approach
        for i in range(min_size, n - min_size):
            before = series_clean.iloc[:i]
            after = series_clean.iloc[i:]
            
            # T-test for mean difference
            if len(before) > 1 and len(after) > 1:
                t_stat, p_val = stats.ttest_ind(before, after)
                if p_val < 0.05:  # Significant change
                    change_points.append(i)  # Return integer index
        
        return change_points
    
    def analyze_shifts_by_product(self):
        """Analyze shifts for each target variable by product type"""
        print("Analyzing shifts by product type...")
        
        products = self.df['Merge'].unique()
        results = {}
        
        for product in products:
            if pd.isna(product):
                continue
                
            product_data = self.df[self.df['Merge'] == product].copy()
            product_results = {}
            
            for target in self.target_vars:
                if target in product_data.columns:
                    series = product_data.set_index('GMT Prod Yield Date')[target]
                    
                    # CUSUM detection
                    pos_shifts, neg_shifts = self.detect_shifts_cusum(series)
                    
                    # Change point detection
                    change_points = self.detect_shifts_changepoint(series)
                    
                    product_results[target] = {
                        'positive_shifts': pos_shifts,
                        'negative_shifts': neg_shifts,
                        'change_points': change_points,
                        'n_observations': len(series.dropna()),
                        'mean_value': series.mean(),
                        'std_value': series.std()
                    }
            
            results[product] = product_results
        
        self.shift_results = results
        return results
    
    def visualize_shifts(self, product=None, target_var=None, save_plots=True):
        """Visualize detected shifts"""
        if product is None:
            products = list(self.shift_results.keys())[:3]  # Show first 3 products
        else:
            products = [product]
        
        if target_var is None:
            targets = self.target_vars[:2]  # Show first 2 targets
        else:
            targets = [target_var]
        
        fig, axes = plt.subplots(len(products), len(targets), 
                                figsize=(15, 5*len(products)))
        if len(products) == 1 and len(targets) == 1:
            axes = np.array([[axes]])
        elif len(products) == 1:
            axes = axes.reshape(1, -1)
        elif len(targets) == 1:
            axes = axes.reshape(-1, 1)
        
        for i, product in enumerate(products):
            product_data = self.df[self.df['Merge'] == product].copy()
            
            for j, target in enumerate(targets):
                ax = axes[i, j] if len(products) > 1 and len(targets) > 1 else axes[i] if len(targets) == 1 else axes[j]
                
                if target in product_data.columns:
                    # Plot time series
                    ax.plot(product_data['GMT Prod Yield Date'], 
                           product_data[target], 'b-', alpha=0.7, label='Data')
                    
                    # Mark shifts - use integer indices to access dates
                    if product in self.shift_results and target in self.shift_results[product]:
                        shifts = self.shift_results[product][target]
                        
                        # Mark positive shifts
                        for shift_idx in shifts['positive_shifts']:
                            if shift_idx < len(product_data):
                                shift_date = product_data.iloc[shift_idx]['GMT Prod Yield Date']
                                ax.axvline(shift_date, color='red', linestyle='--', alpha=0.7, label='Upward Shift')
                        
                        # Mark negative shifts
                        for shift_idx in shifts['negative_shifts']:
                            if shift_idx < len(product_data):
                                shift_date = product_data.iloc[shift_idx]['GMT Prod Yield Date']
                                ax.axvline(shift_date, color='green', linestyle='--', alpha=0.7, label='Downward Shift')
                
                ax.set_title(f'{product} - {target}')
                ax.set_xlabel('Date')
                ax.set_ylabel(target)
                ax.legend()
                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        if save_plots:
            plt.savefig('shift_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_cusum_charts(self, product=None, target_var=None, threshold=3, drift=0.5, save_plots=True, max_products=5):
        """
        Create comprehensive CUSUM charts for each target variable showing positive and negative CUSUM
        Limited to manageable number of products
        """
        # Filter and limit products
        available_products = list(self.shift_results.keys())
        if product is None:
            # Select top products by number of data points
            product_sizes = {}
            for prod in available_products:
                prod_data = self.df[self.df['Merge'] == prod]
                product_sizes[prod] = len(prod_data)
            
            # Sort by data size and take top max_products
            sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
            products = [prod for prod, _ in sorted_products[:max_products]]
        else:
            products = [product] if product in available_products else available_products[:1]
        
        if target_var is None:
            targets = self.target_vars
        else:
            targets = [target_var] if target_var in self.target_vars else self.target_vars
        
        # Create plots for each target variable
        for target in targets:
            if target not in self.df.columns:
                continue
                
            n_products = len(products)
            if n_products == 0:
                continue
                
            fig, axes = plt.subplots(n_products, 3, figsize=(20, 5*n_products))
            if n_products == 1:
                axes = axes.reshape(1, -1)
            
            fig.suptitle(f'CUSUM Analysis for {target} (Top {n_products} Products by Data Volume)', 
                        fontsize=16, fontweight='bold')
            
            for i, product in enumerate(products):
                product_data = self.df[self.df['Merge'] == product].copy()
                
                if target not in product_data.columns or product_data[target].isna().all():
                    continue
                
                # Get clean series with reset index
                series = product_data.set_index('GMT Prod Yield Date')[target].dropna()
                if len(series) < 10:
                    continue
                
                # Standardize
                mean_val = series.mean()
                std_val = series.std()
                if std_val == 0:
                    continue
                
                standardized = (series - mean_val) / std_val
                
                # Calculate CUSUM
                cusum_pos = np.zeros(len(standardized))
                cusum_neg = np.zeros(len(standardized))
                
                for j in range(1, len(standardized)):
                    cusum_pos[j] = max(0, cusum_pos[j-1] + standardized.iloc[j] - drift)
                    cusum_neg[j] = min(0, cusum_neg[j-1] + standardized.iloc[j] + drift)
                
                # Plot 1: Original Time Series with Control Limits
                ax1 = axes[i, 0]
                ax1.plot(series.index, series.values, 'b-', linewidth=1.5, label='Actual Data')
                ax1.axhline(y=mean_val, color='black', linestyle='-', alpha=0.8, label='Mean')
                ax1.axhline(y=mean_val + 3*std_val, color='red', linestyle='--', alpha=0.7, label='UCL (+3Ïƒ)')
                ax1.axhline(y=mean_val - 3*std_val, color='red', linestyle='--', alpha=0.7, label='LCL (-3Ïƒ)')
                ax1.axhline(y=mean_val + 2*std_val, color='orange', linestyle=':', alpha=0.7, label='UWL (+2Ïƒ)')
                ax1.axhline(y=mean_val - 2*std_val, color='orange', linestyle=':', alpha=0.7, label='LWL (-2Ïƒ)')
                
                # Mark detected shifts using integer indices
                if product in self.shift_results and target in self.shift_results[product]:
                    shifts = self.shift_results[product][target]
                    for shift_idx in shifts['positive_shifts']:
                        if shift_idx < len(series):
                            ax1.axvline(series.index[shift_idx], color='red', linestyle='--', 
                                       alpha=0.8, linewidth=2)
                    for shift_idx in shifts['negative_shifts']:
                        if shift_idx < len(series):
                            ax1.axvline(series.index[shift_idx], color='green', linestyle='--', 
                                       alpha=0.8, linewidth=2)
                
                ax1.set_title(f'{product} - {target}\nOriginal Time Series (n={len(series)})')
                ax1.set_ylabel(target)
                ax1.legend(loc='upper right', fontsize=8)
                ax1.grid(True, alpha=0.3)
                plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 2: Positive CUSUM
                ax2 = axes[i, 1]
                ax2.plot(series.index, cusum_pos, 'r-', linewidth=2, label='Positive CUSUM')
                ax2.axhline(y=threshold, color='red', linestyle='--', alpha=0.8, 
                           label=f'Threshold ({threshold})')
                ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                # Highlight threshold exceedances
                above_threshold = cusum_pos > threshold
                if np.any(above_threshold):
                    ax2.fill_between(series.index, 0, cusum_pos, 
                                   where=above_threshold, 
                                   color='red', alpha=0.3, label='Above Threshold')
                
                # Mark shift points
                if product in self.shift_results and target in self.shift_results[product]:
                    for shift_idx in self.shift_results[product][target]['positive_shifts']:
                        if shift_idx < len(series):
                            ax2.axvline(series.index[shift_idx], color='red', linestyle='|', 
                                       alpha=1.0, linewidth=3)
                
                ax2.set_title(f'Positive CUSUM (Upward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax2.set_ylabel('Positive CUSUM')
                ax2.legend(loc='upper right', fontsize=8)
                ax2.grid(True, alpha=0.3)
                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
                
                # Plot 3: Negative CUSUM
                ax3 = axes[i, 2]
                ax3.plot(series.index, cusum_neg, 'g-', linewidth=2, label='Negative CUSUM')
                ax3.axhline(y=-threshold, color='green', linestyle='--', alpha=0.8, 
                           label=f'Threshold (-{threshold})')
                ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                
                # Highlight threshold exceedances
                below_threshold = cusum_neg < -threshold
                if np.any(below_threshold):
                    ax3.fill_between(series.index, 0, cusum_neg, 
                                   where=below_threshold, 
                                   color='green', alpha=0.3, label='Below Threshold')
                
                # Mark shift points
                if product in self.shift_results and target in self.shift_results[product]:
                    for shift_idx in self.shift_results[product][target]['negative_shifts']:
                        if shift_idx < len(series):
                            ax3.axvline(series.index[shift_idx], color='green', linestyle='|', 
                                       alpha=1.0, linewidth=3)
                
                ax3.set_title(f'Negative CUSUM (Downward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax3.set_ylabel('Negative CUSUM')
                ax3.legend(loc='lower right', fontsize=8)
                ax3.grid(True, alpha=0.3)
                plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
                
                # Add product label on the left
                fig.text(0.02, 0.5 - (i/n_products) + 0.5/(2*n_products), f'{product}', 
                        rotation=90, verticalalignment='center', fontsize=12, fontweight='bold')
            
            plt.tight_layout()
            if save_plots:
                plt.savefig(f'cusum_analysis_{target.replace(" ", "_").replace("%", "pct")}.png', 
                           dpi=300, bbox_inches='tight')
            plt.show()
    
    def plot_cusum_summary(self, save_plots=True, max_products=8):
        """
        Create a summary CUSUM chart showing all target variables for comparison
        Limited to top products by data volume
        """
        # Select top products by data volume
        product_sizes = {}
        for product in self.shift_results.keys():
            prod_data = self.df[self.df['Merge'] == product]
            product_sizes[product] = len(prod_data)
        
        sorted_products = sorted(product_sizes.items(), key=lambda x: x[1], reverse=True)
        top_products = [prod for prod, _ in sorted_products[:max_products]]
        
        fig, axes = plt.subplots(len(self.target_vars), 1, figsize=(15, 4*len(self.target_vars)))
        if len(self.target_vars) == 1:
            axes = [axes]
        
        fig.suptitle(f'CUSUM Summary - All Target Variables (Top {len(top_products)} Products)', 
                    fontsize=16, fontweight='bold')
        
        colors = plt.cm.tab10(np.linspace(0, 1, len(top_products)))
        
        for i, target in enumerate(self.target_vars):
            if target not in self.df.columns:
                continue
            
            ax = axes[i]
            
            # Plot CUSUM for each product on the same chart
            for j, product in enumerate(top_products):
                product_data = self.df[self.df['Merge'] == product].copy()
                
                if target not in product_data.columns or product_data[target].isna().all():
                    continue
                
                series = product_data.set_index('GMT Prod Yield Date')[target].dropna()
                if len(series) < 10:
                    continue
                
                # Calculate standardized CUSUM
                mean_val = series.mean()
                std_val = series.std()
                if std_val == 0:
                    continue
                
                standardized = (series - mean_val) / std_val
                cusum_pos = np.zeros(len(standardized))
                
                for k in range(1, len(standardized)):
                    cusum_pos[k] = max(0, cusum_pos[k-1] + standardized.iloc[k] - 0.5)
                
                color = colors[j]
                ax.plot(series.index, cusum_pos, color=color, linewidth=1.5, 
                       alpha=0.8, label=f'{product} (n={len(series)})')
            
            ax.axhline(y=3, color='red', linestyle='--', alpha=0.8, label='Threshold (3)')
            ax.set_title(f'{target} - CUSUM Comparison by Product')
            ax.set_ylabel('Positive CUSUM')
            ax.legend(loc='upper right', fontsize=8, ncol=2)
            ax.grid(True, alpha=0.3)
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        if save_plots:
            plt.savefig('cusum_summary_all_targets.png', dpi=300, bbox_inches='tight')
        plt.show()(series.index, 0, cusum_neg, 
                                   where=(cusum_neg < -threshold), 
                                   color='green', alpha=0.3, label='Below Threshold')
                
                # Mark shift points
                if product in self.shift_results and target in self.shift_results[product]:
                    for shift_idx in self.shift_results[product][target]['negative_shifts']:
                        if shift_idx < len(series):
                            ax3.axvline(series.index[shift_idx], color='green', linestyle='|', 
                                       alpha=1.0, linewidth=3, markersize=10,
                                       label='Detected Shift' if shift_idx == self.shift_results[product][target]['negative_shifts'][0] else "")
                
                ax3.set_title(f'Negative CUSUM (Downward Shifts)\nDrift={drift}, Threshold={threshold}')
                ax3.set_ylabel('Negative CUSUM')
                ax3.legend(loc='lower right', fontsize=8)
                ax3.grid(True, alpha=0.3)
                plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
                
                # Add product label on the left
                fig.text(0.02, 0.5 - (i/n_products) + 0.5/(2*n_products), f'{product}', 
                        rotation=90, verticalalignment='center', fontsize=12, fontweight='bold')
            
            plt.tight_layout()
            if save_plots:
                plt.savefig(f'cusum_analysis_{target.replace(" ", "_").replace("%", "pct")}.png', 
                           dpi=300, bbox_inches='tight')
            plt.show()
    
    def plot_cusum_summary(self, save_plots=True):
        """
        Create a summary CUSUM chart showing all target variables for comparison
        """
        fig, axes = plt.subplots(len(self.target_vars), 1, figsize=(15, 4*len(self.target_vars)))
        if len(self.target_vars) == 1:
            axes = [axes]
        
        fig.suptitle('CUSUM Summary - All Target Variables', fontsize=16, fontweight='bold')
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, target in enumerate(self.target_vars):
            if target not in self.df.columns:
                continue
            
            ax = axes[i]
            
            # Plot CUSUM for each product on the same chart
            for j, product in enumerate(list(self.shift_results.keys())[:5]):  # Max 5 products
                product_data = self.df[self.df['Merge'] == product].copy()
                
                if target not in product_data.columns or product_data[target].isna().all():
                    continue
                
                series = product_data.set_index('GMT Prod Yield Date')[target].dropna()
                if len(series) < 10:
                    continue
                
                # Calculate standardized CUSUM
                mean_val = series.mean()
                std_val = series.std()
                if std_val == 0:
                    continue
                
                standardized = (series - mean_val) / std_val
                cusum_pos = np.zeros(len(standardized))
                
                for k in range(1, len(standardized)):
                    cusum_pos[k] = max(0, cusum_pos[k-1] + standardized.iloc[k] - 0.5)
                
                color = colors[j % len(colors)]
                ax.plot(series.index, cusum_pos, color=color, linewidth=1.5, 
                       alpha=0.8, label=f'{product}')
            
            ax.axhline(y=3, color='red', linestyle='--', alpha=0.8, label='Threshold (3)')
            ax.set_title(f'{target} - CUSUM Comparison by Product')
            ax.set_ylabel('Positive CUSUM')
            ax.legend(loc='upper right', fontsize=8)
            ax.grid(True, alpha=0.3)
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        if save_plots:
            plt.savefig('cusum_summary_all_targets.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def correlate_shifts_with_variables(self, window_days=7):
        """
        Correlate detected shifts with process/material/prestep variable changes
        """
        print("Correlating shifts with variable changes...")
        
        correlation_results = {}
        
        for product, product_shifts in self.shift_results.items():
            product_data = self.df[self.df['Merge'] == product].copy()
            product_correlations = {}
            
            for target, shift_info in product_shifts.items():
                all_shifts = (shift_info['positive_shifts'] + 
                             shift_info['negative_shifts'] + 
                             shift_info['change_points'])
                
                if not all_shifts:
                    continue
                
                # Analyze variable changes around shift points
                variable_correlations = self.analyze_variable_changes_around_shifts(
                    product_data, all_shifts, window_days)
                
                product_correlations[target] = variable_correlations
            
            correlation_results[product] = product_correlations
        
        self.correlation_results = correlation_results
        return correlation_results
    
    def analyze_variable_changes_around_shifts(self, data, shift_indices, window_days=7):
        """Analyze how variables change around shift points"""
        variable_importance = {}
        
        # Get all potential predictor variables
        all_vars = (self.process_vars + self.material_vars + self.prestep_vars + 
                   [col for col in data.columns if '_lag' in col or 'X14_' in col or 'X17_' in col])
        all_vars = [var for var in all_vars if var in data.columns]
        
        for var in all_vars:
            if data[var].dtype in ['object', 'string']:
                continue
                
            changes = []
            for shift_idx in shift_indices:
                if shift_idx < window_days or shift_idx >= len(data) - window_days:
                    continue
                
                before = data.iloc[shift_idx-window_days:shift_idx][var].mean()
                after = data.iloc[shift_idx:shift_idx+window_days][var].mean()
                
                if pd.notna(before) and pd.notna(after) and before != 0:
                    pct_change = abs((after - before) / before) * 100
                    changes.append(pct_change)
            
            if changes:
                variable_importance[var] = {
                    'mean_change': np.mean(changes),
                    'max_change': np.max(changes),
                    'frequency': len(changes) / len(shift_indices) if shift_indices else 0
                }
        
        # Sort by importance score (combination of magnitude and frequency)
        for var, stats in variable_importance.items():
            stats['importance_score'] = stats['mean_change'] * stats['frequency']
        
        sorted_vars = sorted(variable_importance.items(), 
                           key=lambda x: x[1]['importance_score'], reverse=True)
        
        return dict(sorted_vars[:20])  # Top 20 most important variables
    
    def build_predictive_models(self):
        """Build random forest models to predict target variables"""
        print("Building predictive models...")
        
        models = {}
        feature_importance = {}
        
        for target in self.target_vars:
            if target not in self.df.columns:
                continue
            
            print(f"Building model for {target}...")
            
            # Prepare features
            feature_cols = []
            for col in self.df.columns:
                if (col.startswith('X') and col != target and 
                    self.df[col].dtype in [np.number] and 
                    not col.endswith('_x') and not col.endswith('_y')):
                    feature_cols.append(col)
            
            # Create feature matrix
            X = self.df[feature_cols].copy()
            y = self.df[target].copy()
            
            # Remove rows with missing target
            mask = ~y.isna()
            X = X[mask]
            y = y[mask]
            
            if len(y) < 50:  # Not enough data
                continue
            
            # Handle missing values in features
            X = X.fillna(X.median())
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42)
            
            # Train model
            rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
            rf.fit(X_train, y_train)
            
            # Evaluate
            y_pred = rf.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            models[target] = {
                'model': rf,
                'mse': mse,
                'r2': r2,
                'features': feature_cols
            }
            
            # Feature importance
            feature_imp = pd.DataFrame({
                'feature': feature_cols,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=False)
            
            feature_importance[target] = feature_imp
        
        return models, feature_importance
    
    def generate_summary_report(self):
        """Generate comprehensive summary report"""
        print("\n" + "="*80)
        print("OEE SHIFT ANALYSIS SUMMARY REPORT")
        print("="*80)
        
        # Shift summary by product
        print("\n1. SHIFT DETECTION SUMMARY BY PRODUCT:")
        print("-" * 50)
        
        for product, product_results in self.shift_results.items():
            print(f"\nProduct: {product}")
            for target, shift_info in product_results.items():
                total_shifts = (len(shift_info['positive_shifts']) + 
                              len(shift_info['negative_shifts']) + 
                              len(shift_info['change_points']))
                print(f"  {target}: {total_shifts} shifts detected "
                      f"(Mean: {shift_info['mean_value']:.2f}, Std: {shift_info['std_value']:.2f})")
        
        # Top correlations
        print("\n2. TOP VARIABLE CORRELATIONS WITH SHIFTS:")
        print("-" * 50)
        
        all_correlations = {}
        for product, targets in self.correlation_results.items():
            for target, variables in targets.items():
                for var, stats in variables.items():
                    key = f"{product}_{target}_{var}"
                    all_correlations[key] = stats['importance_score']
        
        # Top 10 correlations
        top_correlations = sorted(all_correlations.items(), 
                                key=lambda x: x[1], reverse=True)[:10]
        
        for i, (key, score) in enumerate(top_correlations, 1):
            product, target, var = key.rsplit('_', 2)
            print(f"{i:2d}. {var} -> {target} ({product}): Score = {score:.2f}")
        
        # Recommendations
        print("\n3. RECOMMENDATIONS:")
        print("-" * 50)
        print("Based on the analysis, focus on:")
        
        # Get most problematic products
        product_shift_counts = {}
        for product, targets in self.shift_results.items():
            total_shifts = sum(len(info['positive_shifts']) + len(info['negative_shifts']) + 
                             len(info['change_points']) for info in targets.values())
            product_shift_counts[product] = total_shifts
        
        top_products = sorted(product_shift_counts.items(), 
                            key=lambda x: x[1], reverse=True)[:3]
        
        for i, (product, shift_count) in enumerate(top_products, 1):
            print(f"{i}. Product '{product}' with {shift_count} total shifts")
        
        # Most important variables
        print(f"\n4. MONITOR THESE VARIABLES CLOSELY:")
        top_vars = set()
        for key, score in top_correlations[:5]:
            var = key.rsplit('_', 2)[2]
            top_vars.add(var)
        
        for var in top_vars:
            print(f"   - {var}")

# Usage example:
def run_complete_analysis(data_path_or_df):
    """
    Complete analysis pipeline
    """
    # Initialize analyzer
    analyzer = OEEShiftAnalyzer(data_path_or_df) if isinstance(data_path_or_df, str) else OEEShiftAnalyzer(df=data_path_or_df)
    
    # Step 1: Preprocess data
    analyzer.preprocess_data()
    
    # Step 2: Detect shifts by product
    analyzer.analyze_shifts_by_product()
    
    # Step 3: Create comprehensive CUSUM plots
    print("Generating CUSUM charts for all target variables...")
    analyzer.plot_cusum_charts()
    
    # Step 4: Create CUSUM summary comparison
    analyzer.plot_cusum_summary()
    
    # Step 5: Visualize basic shifts (original method)
    analyzer.visualize_shifts()
    
    # Step 6: Correlate shifts with variables
    analyzer.correlate_shifts_with_variables()
    
    # Step 7: Build predictive models
    models, feature_importance = analyzer.build_predictive_models()
    
    # Step 8: Generate report
    analyzer.generate_summary_report()
    
    return analyzer, models, feature_importance

def run_cusum_analysis_only(data_path_or_df, target_var=None, product=None):
    """
    Run only CUSUM analysis for quick shift detection
    """
    analyzer = OEEShiftAnalyzer(data_path_or_df) if isinstance(data_path_or_df, str) else OEEShiftAnalyzer(df=data_path_or_df)
    analyzer.preprocess_data()
    analyzer.analyze_shifts_by_product()
    
    # Generate CUSUM plots
    analyzer.plot_cusum_charts(product=product, target_var=target_var)
    analyzer.plot_cusum_summary()
    
    return analyzer

# To use this code:
# analyzer, models, feature_importance = run_complete_analysis('your_data.csv')
# or
# analyzer, models, feature_importance = run_complete_analysis(your_dataframe)
